\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage[colorlinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{csvsimple}
\usepackage{luacode}
\setlength{\droptitle}{-6em}

% Enter the specific assignment number and topic of that assignment below, and replace "Your Name" with your actual name.

\title{\textbf{COMP0078 Assignment 2}}
\author{Student Numbers: 21168615 \& 19004608 \\ }
\date{Dec 14, 2022}

\begin{document}
    \maketitle
\section{PART I}
\subsection{Kernel Perceptron}
\subsubsection{Experimental Results}

\begin{itemize}
    \item[1.] Basic Results:

    \includegraphics{outputs/part1/q1.png}


    \item[2.] Cross Validation Results:

    \includegraphics{outputs/part1/q2.png}

    \item[3.] Confusion Matrix:

    \includegraphics{outputs/part1/q3_confusion.png}
\newpage
    \item[4.] Hardest to predict images:

    \includegraphics{outputs/part1/q4.png}
    \newpage
    \item[5.] Basic Results:

    \includegraphics{outputs/part1/q5_1.png}

    Cross Validation Results:

    \includegraphics{outputs/part1/q5_2.png}
\end{itemize}
\subsubsection{Discussions}
    Choice of Parameters that weren't cross validated

    Generalisation to k-classifiers

    Kernel Comparison

    Kernel Perceptron Implementation


\newpage
\section{PART II}
\subsection{Semi-supervised Learning via Laplacian Interpolation}

Experimental report for the laplacian interpolation approach:
\\

\includegraphics{outputs/part2/laplacian_interpolation_report.png}

\\

And for the laplacian kernel method:

\includegraphics{outputs/part2/laplacian_kernel_interpolation_report.png}

Some observations to make here are:\\


a) Both models seem to perform fairly well, even in the low-data setting.\\

b) Increating the number of known labels increases the accuracy and decreases
 the variance of the predictors.\\

c) Increasing the total number of datapoints generally increases both model accuracies,
and decreases variance. \\

d) The laplacian kernel method outperforms the vanilla inerpolation approach considerably,
on both accuracy and variance. \\


The main reason for the success of this algorithm is the high degree of
 cluster separation observed in the dataset.
\\

For illustration, here is a diagram representing the graph adjacency matrix,
 with colours showing the labelling of an edge. Here blue edges are where both 
labels are -1, teal when both are different, and yellow when both are +1.\\


\includegraphics{outputs/part2/graph_label_diagram.png}#

We see through this that the vast majority of datapoints are connected only to points
of the same labelling, and that the graph is highly separated  into 2 clusters.
Hence with minimal training information, both methods are able to predict with a
 high degree of accuracy.\\

Note that error variance decreases as a function of the number of known labels, but 
has a muhc more significant impacgt for the vanilla interpolation method.


We see that the kernel approach consistently outperforms the simple laplacian
 interpolation method. A reasonable explanation for this is that the laplacian
interpolation approach utilises local information to 'diffuse' labels through the graph. 
This means that only datapoints close to labelled data receive information from the 
labels themselves, and accuracy is likely reduced for datapoints far from the labels.
In contrast, the kernel interpolation method takes global information from all the 
labelled datapoints and weights them according to their proximity and connectedness 
to eachother to predict labels.
\newpage



\section{PART III}
\subsection{Questions}
\begin{itemize}
    \item[a.]
    \item[b.]
    \item[c.]
    \item[D.]\\
    We note that since for any $X_{t}$ , $y_{t} = X_{1,t}$ , we have the following
     expression representing the linear separability of our dataset:\\
    \[(v \cdot x_{t})y_{t} \ge 1\]\\
    Where $v = (1,0, \dots, 0)^{T}$. Note that $\|v\| =1$.\\
    Hence, in our online mistake bound for the perceptron, we have that $\gamma =1$.
    Further note that $\forall t$, $x_{t} \in {-1,1}^{n} \implies \|x_{t}\|^{2}
    = n$. This gives us that $R = max_{t} \|x_{t}\| = \sqrt{n}$.
    Hence our mistake bound for the perceptron algorithm is given by:\\
    
    \[M \le n \].\\

    Using the theorem on page 60 of the online learning notes, we arrive at:\\

    \[Prob(\mathcal{A_{S}}(x ^{\prime}) \ne y^{\prime} ) \le \frac{n}{m}\]\\

     \item[E.]\\
     From our experimental observations we may expect the sample complexity of 1-NN
     to be lower bounded by some exponential function. We formalise this in the
     following proposition:\\

    \textbf{Proposition:}\\

    Following the data-generating distribution described above, we have that our sample complexity given by:\\

    $m(n) = \Omega (n)$ \\
    \textbf{Proof:}\\

    Suppose we sample a training set $S = \{(x_{1},y_{1}), \dots, (x_{n},y_{n})\} $ uniformly from the set $\{-1,1\}^{n}$, with associated labels
    defined by the rule $y | x = x_{1}$, and use this training set for inference on an
     arbitrary test point $(x,x_{1})$, sampled uniformly from the same set.
    \\
    We note the following observation:\\

    \textbf{Obs:}\\
    if $x \in S$, then $\mathcal{A}_{S}(x)) = y$, where y is the true label for x. \\

    To justify this, observe that our dataset represents a realiseable learning problem,
    and as such, if $(x,y), (x \prime,y \prime)$ are datapoints sampled from our distribution,
    $x = x \prime \implies y = y \prime $. Trivially, x is the 1-nearest neighbour to itself,
    so the algorithm makes a correct prediction for any datapoint present in our training set.\\

    Hence, $\mathcal{A}_{S}$ makes an error on x $\implies x \notin S$.
    
    Hence, the set of all training sets that make an error on x is contained in the set of all
    training sets not containing x. \\

    Hence, for a given x, $P_{S}(\mathcal{A}_{S}(x) \ne y) \le P_{S}(x \notin S)$.\\

    Since S is a collection of points sampled iid from our data generating distribution, \\
    $P_{S}(x \notin S) = \prod_{i=1}^{m}P(x_{i} \ne x) = \prod_{i}(1 - P(x_{i} = x)) = (1 - 2^{-n})^{m}$\\

    We note that our choice of x was arbitrary, and since we sampled x uniformly, we arrive at the 
    following generalisation error bound:\\

    $\mathbb{E}_{S \sim \mathcal{D}^{m}, x \sim \mathcal{D}}[\mathcal{L}_{S}(x)] \le (1 - 2^{-n})^{m} $\\
    Using the identity $1-x \le e^{-x}$ provided frequently in the notes, we simplify this bound to give:
    \\
    \\

    $\mathbb{E}_{S \sim \mathcal{D}^{m}, x \sim \mathcal{D}}[\mathcal{L}_{S}(x)] \le exp(-2^{n}m)$
    \\
    Suppose we seek m such that our generalisation error is less than some fixed $\epsilon$. \\
    \\
    Hence we require $\mathbb{E}_{S \sim \mathcal{D}^{m}, x \sim \mathcal{D}}[\mathcal{L}_{S}(x)] \le exp(-2^{n}m) \le \epsilon$\\
    \\
    $\implies -m2^{-n} \le log(\epsilon)$\\

    Hence, $m \ge -log(\epsilon)2^{n} = log(\frac{1}{\epsilon}) 2^{n}$\\

    $\implies m = \Omega(2^{n})$\\
    $\square$
\end{itemize}
\end{document}
